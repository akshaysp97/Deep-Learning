# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kV1hUKvrePmYQR79ATtavHzXw_qNBtlT
"""

import librosa
import os 
import numpy as np
import math
import h5py
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import plot_model
from tensorflow.keras.layers import BatchNormalization,Activation
from tensorflow.keras import regularizers 
from tensorflow.keras import layers, optimizers
from tensorflow.keras.layers import Input, Dense, GRU
from tensorflow.keras import Model,Sequential
import matplotlib.pyplot as plt

np.random.seed(0)
eng_file = 'train_english';
hin_file = 'train_hindi';
man_file = 'train_mandarin';

def Samples(filename):
    lst = os.listdir(filename)
    lst_files = list()
    for i in lst:
        path = os.path.join(filename, i)
        if os.path.isdir(path):
            lst_files = lst_files + Samples(path)
        else:
            lst_files.append(path)       
    return lst_files

def language(a,b,direct):
  temp = True
  temp1 = True
  s = 150
  dimn = 64
  list1 = []
  list2 = []
  count = 0
  for f in direct:
    if temp and count < a:  
      y,sr = librosa.load(f,sr=16000)
      inter_20 = librosa.effects.split(y, top_db=20)
      above_20 = np.zeros(y.shape)
      for i in inter_20:
        start,end = i
        above_20[start:end]=y[start:end]
      y1 = []
      for i in range(len(above_20)):
        if above_20[i]>0.1:
          y1.append(above_20[i])
      y1 = np.array(y1)
      mat = librosa.feature.mfcc(y=y1,sr=sr,n_mfcc=64,n_fft=int(sr*0.025),hop_length=int(sr*0.01))
      n = mat.shape[1]%s
      num_seqs = int(math.floor(mat.shape[1]/s))
      mat1 = np.transpose(mat)
      if n!=0:
        mat1 = mat1[:-n,:]
      mat2 = mat1.reshape(num_seqs, s,dimn)
      list1.append(y)
      list2.append(mat2)
      count+=1
      if temp1 and count>=b and count<(b+3):
        y,sr = librosa.load(f,sr=16000)
        mat = librosa.feature.mfcc(y=y,sr=sr,n_mfcc=64,n_fft=int(sr*0.025),hop_length=int(sr*0.01))
        n = mat.shape[1]%s
        num_seqs = int(math.floor(mat.shape[1]/s))
        mat1 = np.transpose(mat)
        if n !=0:
          mat1 = mat1[:-n,:]
        mat2 = mat1.reshape(num_seqs, s,dimn)
        list1.append(y)
        list2.append(mat2)
        count+=1
      
      return list2

lsts_english = Samples(eng_file)
lsts_hindi = Samples(hin_file)
lsts_mandarin = Samples(man_file)
lsts_english.sort()
lsts_hindi.sort()
lsts_mandarin.sort()
mat_eng = language(29,29,lsts_english)
mat_hin = language(27,25,lsts_hindi)

s = 150
dimn = 64
temp = True
temp1 = True
lst_man = []
lst1_man = []
count = 0
for f in lsts_mandarin:
  if temp and count<32:  
    y,sr = librosa.load(f,sr=16000)
    inter_20 = librosa.effects.split(y, top_db=20)
    above_20 = np.zeros(y.shape)

    for i in inter_20:
      start,end = i
      above_20[start:end]=y[start:end]
    y1 = []
    for i in range(len(above_20)):
      if above_20[i]>0.1:
        y1.append(above_20[i])
    y1 = np.array(y1)
    mat = librosa.feature.mfcc(y=y1,sr=sr,n_mfcc=64,n_fft=int(sr*0.025),hop_length=int(sr*0.01))
    n = mat.shape[1]%s
    num_seqs = int(math.floor(mat.shape[1]/s))
    mat1 = np.transpose(mat)
    if n!=0:
      mat1 = mat1[:-n,:]
    mat2 = mat1.reshape(num_seqs, s,dimn)
    lst_man.append(y)
    lst1_man.append(mat2)
    count+=1

  if temp1 and count>=29 and count<32:
    y,sr = librosa.load(f,sr=16000)
    mat = librosa.feature.mfcc(y=y,sr=sr,n_mfcc=64,n_fft=int(sr*0.025),hop_length=int(sr*0.01))
    n = mat.shape[1]%s
    num_seqs = int(math.floor(mat.shape[1]/s))
    mat1 = np.transpose(mat)
    if n!=0:
      mat1 = mat1[:-n,:]
    mat2 = mat1.reshape(num_seqs, s,dimn)
    lst_man.append(y)
    lst1_man.append(mat2)
    count+=1


def concat(lst,i,j,k):
  s = 150
  concat = np.concatenate(lst[0:29],axis=0)
  arr = [i,j,k]
  r, c = (concat.shape[0], s) 
  labels = [[arr for i in range(c)] for j in range(r)]
  return concat,labels

concat_eng,eng_labels = concat(mat_eng,1,0,0)
concat_hin,hin_labels = concat(mat_hin,0,1,0)
concat_man,man_labels = concat(lst1_man,0,0,1)

train_eng, val_eng, ytrain_eng, yval_eng = train_test_split(concat_eng, eng_labels, test_size=0.3, shuffle = False)
train_hin, val_hin, ytrain_hin, yval_hin  = train_test_split(concat_hin, hin_labels, test_size=0.3, shuffle = False)
train_man, val_man, ytrain_man, yval_man = train_test_split(concat_man, man_labels, test_size=0.3, shuffle = False)

train_data = np.concatenate((train_eng,train_hin,train_man),axis=0)
X_val = np.concatenate((val_eng,val_hin,val_man),axis=0)
y_train = np.vstack((ytrain_eng,ytrain_hin,ytrain_man))
y_val = np.vstack((yval_eng,yval_hin,yval_man))
validation_data = (X_val,y_val)

def GRU_Model(alpha):
  model = Sequential()
  model.add(Dense(16, activation='relu', input_shape=(s,64),kernel_regularizer=regularizers.l2(alpha),
                        activity_regularizer=regularizers.l1(alpha)))
  model.add(GRU(16, return_sequences=True, stateful=False,kernel_regularizer=regularizers.l2(alpha),
                        activity_regularizer=regularizers.l1(alpha)))
  model.add(Dense(32, activation='relu', input_shape=(s,64),kernel_regularizer=regularizers.l2(alpha),
                        activity_regularizer=regularizers.l1(alpha)))
  model.add(Dense(3, activation='softmax'))
  model.compile(loss='mean_squared_error', optimizer='adam',metrics=['accuracy'])
  return model

model = GRU_Model(0.00001)
model.summary()

history = model.fit(train_data, y_train, batch_size=16, epochs=30,shuffle=True,validation_data = validation_data)
model.save('GRU_model_silence_removed.hdf5')
model.save_weights('weights.hd5')

plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True,
    rankdir='TB', expand_nested=False, dpi=96)

def plot_history(history):
    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]
    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]
    acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' not in s]
    val_acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' in s]
   
    if len(loss_list) == 0:
        print('Loss is missing in history')
        return
   
    epochs = range(1,len(history.history[loss_list[0]]) + 1)
   
    plt.figure(1)
    for l in loss_list:
        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))
    for l in val_loss_list:
        plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))
   
    plt.title('Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
   
    plt.figure(2)
    for l in acc_list:
        plt.plot(epochs, history.history[l], 'b', label='Training accuracy (' + str(format(history.history[l][-1],'.5f'))+')')
    for l in val_acc_list:    
        plt.plot(epochs, history.history[l], 'g', label='Validation accuracy (' + str(format(history.history[l][-1],'.5f'))+')')

    plt.title('Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

plot_history(history)

def Streaming_Model(alpha):
  streaming_model = Sequential()
  streaming_model = Sequential()
  streaming_model.add(Dense(16, activation='relu',batch_input_shape=(1,None,64),kernel_regularizer=regularizers.l2(alpha),
                          activity_regularizer=regularizers.l1(alpha)))
  streaming_model.add(GRU(16, return_sequences=False, stateful=True))
  streaming_model.add(Dense(32, activation='relu',batch_input_shape=(1,None,64),kernel_regularizer=regularizers.l2(alpha),
                          activity_regularizer=regularizers.l1(alpha)))
  streaming_model.add(Dense(3, activation='softmax'))
  streaming_model.compile(loss='mean_squared_error', optimizer='adam',metrics=['accuracy'])
  return streaming_model

streaming_model = Streaming_Model(0.00001)
streaming_model.summary()
streaming_model.load_weights('weights.hd5')
streaming_model.save('streaming_model')
print('streaming model loaded')

plot_model(streaming_model, to_file='streaming_model.png', show_shapes=True, show_layer_names=True,
    rankdir='TB', expand_nested=False, dpi=96)